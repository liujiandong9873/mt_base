{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TASK_NAME = '_75%' # postfix to all train and test file names \n",
    "\n",
    "SNIPPET_DIR_PATH = './data/snippet/only_snippet/'\n",
    "ORIGIN_DIR_PATH = './data/_original_data/'\n",
    "\n",
    "\n",
    "TRAIN_NAME = f'train{SPECIAL_TASK_NAME}'\n",
    "\n",
    "TEST_17_NAME = f'test_17{SPECIAL_TASK_NAME}'\n",
    "TEST_18_NAME = f'test_18{SPECIAL_TASK_NAME}'\n",
    "TRAIN_3000_NAME = f'train_3000{SPECIAL_TASK_NAME}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_train = pd.read_excel(f'{SNIPPET_DIR_PATH}textbook_train.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_test_17 = pd.read_excel(f'{SNIPPET_DIR_PATH}textbook_test_17.xlsx', header=None)\n",
    "textbook_test_18 = pd.read_excel(f'{SNIPPET_DIR_PATH}textbook_test_18.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_excel(f'{ORIGIN_DIR_PATH}train.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_17 = pd.read_excel(f'{ORIGIN_DIR_PATH}test_17-18.xlsx', sheet_name='2017', header=None)\n",
    "test_data_18 = pd.read_excel(f'{ORIGIN_DIR_PATH}test_17-18.xlsx', sheet_name='2018', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_snippet = pd.concat([train_data, textbook_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_snippet.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_with_snippet = pd.concat([test_data_17, textbook_test_17], axis=1)\n",
    "test_18_with_snippet = pd.concat([test_data_18, textbook_test_18], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the corruyted entry! other data does not have col=8-9\n",
    "train_with_snippet[train_with_snippet.iloc[:,8].isnull() != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_snippet[train_with_snippet.iloc[:,8].isnull() != True].iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_with_snippet[train_with_snippet.iloc[:,8].isnull() != True].iloc[:,0].tolist() != []:\n",
    "    print(train_with_snippet[train_with_snippet.iloc[:,8].isnull() != True].iloc[:,0].tolist())\n",
    "    train_with_snippet.drop(train_with_snippet.index[10040], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.iloc[10040, :]\n",
    "# train_data.drop(train_data.index[10040], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data.iloc[:, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.isnull().stack()[lambda x: x].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check corruyted data again\n",
    "train_with_snippet[train_with_snippet.iloc[:,8].isnull() != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: question\n",
    "# 1: question type\n",
    "# 2-6: choices\n",
    "# 7: answer\n",
    "# 10-14: textbook snippet\n",
    "\n",
    "train_data = train_with_snippet.iloc[:,[0,1,2,3,4,5,6,7,10,11,12,13,14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning - drop nan entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop entries that has any nan and, \n",
    "# print num of nan in each col\n",
    "def drop_nan(df):\n",
    "    if df.isnull().values.any():\n",
    "        print(\"nan in each col:\\n\", df.isnull().sum(), sep='')\n",
    "        \n",
    "        return df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = drop_nan(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index after dropping rows\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data = train_data.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns = ['q', 'qtype', 'c1', 'c2', 'c3', 'c4', 'c5', 'a', 's1', 's2', 's3', 's4', 's5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_with_snippet.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_with_snippet.columns = ['q','c1','c2','c3','c4','c5', 'qtype', 'year', 'a', 's1','s2','s3','s4','s5']\n",
    "test_18_with_snippet.columns = ['q','c1','c2','c3','c4','c5', 'qtype', 'year', 'a', 's1','s2','s3','s4','s5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_with_snippet.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_18_with_snippet.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_len = [len(i) for i in train_data['q']]\n",
    "s1_len = [len(i) for i in train_data['s1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(s1_len, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(q_len, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add textbook snippit - only the best match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. question snippet in one sentence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question and snippet being combined into one pandas series\n",
    "# add the best (the first) match snippet from textbook \n",
    "train_combined_qs = train_with_snippet.iloc[:,0].combine(train_with_snippet.iloc[:,10], func=(lambda q, t: str(q) + str(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_qs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question and snippet in one sentence \n",
    "train_data_combined_qs = train_data.iloc[:,:8].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined_qs.iloc[:,0] = train_combined_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined_qs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do the same procedure for testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_18_with_snippet.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff from training\n",
    "# best match is at col index 9\n",
    "test_17_combined_qs = test_17_with_snippet.iloc[:,0].combine(test_17_with_snippet.iloc[:,9], func=(lambda q, t: str(q) + str(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_18_combined_qs = test_18_with_snippet.iloc[:,0].combine(test_18_with_snippet.iloc[:,9], func=(lambda q, t: str(q) + str(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_with_snippet.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_17_combined_qs = test_17_with_snippet.iloc[:, :9].copy()\n",
    "test_data_18_combined_qs = test_18_with_snippet.iloc[:, :9].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_17_combined_qs.iloc[:,0] = test_17_combined_qs\n",
    "test_data_18_combined_qs.iloc[:,0] = test_18_combined_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double checking \n",
    "test_data_17_combined_qs.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. question snippet in separate sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best match is in the last column\n",
    "train_data_separate_qs = train_data.iloc[:,:9].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_separate_qs.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: same process for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_17_separate_qs = test_17_with_snippet.iloc[:, :10].copy()\n",
    "test_data_18_separate_qs = test_18_with_snippet.iloc[:, :10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_17_separate_qs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data_18_separate_qs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_3000_separate_qs = train_data_separate_qs.head(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_3000_separate_qs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data \n",
    "\n",
    "\n",
    "\n",
    "question + 5 choice -> 5 * (question + 1 choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, question_index, first_choice_index, answer_index):\n",
    "    \"\"\"\n",
    "    question_index: int, specify the index of question column \n",
    "    first_choice_index: int, the index of the *first* multiple choice column among 5\n",
    "    answer_index: int, index of answer column\n",
    "    \n",
    "    return:\n",
    "    expanded\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # iterate through all entries in df\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        one_entry = df.iloc[i,:] \n",
    "        # for each entry, take its 5 choices in sequence into 5 [question, one_choice, label] outputs\n",
    "        for choice_index in range(first_choice_index, first_choice_index + 5):\n",
    "            \n",
    "            label = 1 if (choice_index - first_choice_index + 1)  == one_entry[answer_index] else 0\n",
    "            result.append({'question': one_entry[question_index], \n",
    "                       'choice': one_entry[choice_index], \n",
    "                       'label': label})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_combined_qs = pd.DataFrame(prepare_data(train_data_combined_qs, 0, 2, 7))\n",
    "# this step is extramely slow, add converting process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3000_df_combined_qs = train_df_combined_qs.head(15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3000_df_combined_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_df_combined_qs = pd.DataFrame(prepare_data(test_data_17_combined_qs, 0, 1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_df_combined_qs.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_18_df_combined_qs = pd.DataFrame(prepare_data(test_data_18_combined_qs, 0, 1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_snippet(snippet, remaining_len, overlap_rate):\n",
    "    \"\"\"\n",
    "    snippet: str, a textbook snippet from best matching results\n",
    "    remaining_len: int, range: [0,1], available token length after minus ----\n",
    "    overlap_rate: float, overlap rate between each snippet pieces\n",
    "    \n",
    "    return: a list of snippet pieces\n",
    "    \"\"\"\n",
    "    snippet = str(snippet)\n",
    "    snippet_len = len(snippet)\n",
    "    assert remaining_len > 0, 'Remaining length <= 0'\n",
    "    assert overlap_rate > 0 and overlap_rate <= 1, 'Overlap rate should be within [0,1]'\n",
    "    \n",
    "    # if snippet can fit into the remaining length\n",
    "    if snippet_len <= remaining_len:\n",
    "        return [snippet]\n",
    "\n",
    "    # if snippet is too long \n",
    "    piece_size = int(remaining_len * (1-overlap_rate)) # floor\n",
    "    piece_nums = snippet_len // piece_size + 1\n",
    "    piece_nums_per_iteration = int(1/(1-overlap_rate))\n",
    "\n",
    "\n",
    "    piece_iteration_nums = piece_nums - int(1/(1-overlap_rate)) + 1\n",
    " \n",
    "\n",
    "    piece_list = []\n",
    "    for c_i in range(piece_iteration_nums):\n",
    "        piece_list.append(snippet[c_i*piece_size : c_i*piece_size + remaining_len - 1])\n",
    "    return piece_list\n",
    "\n",
    "def prepare_data_overlapped(df, question_index, first_choice_index, answer_index, max_length=512, overlap_rate=0.75):\n",
    "    \"\"\"\n",
    "    question_index: int, specify the index of question column \n",
    "    first_choice_index: int, the index of the *first* multiple choice column among 5\n",
    "    answer_index: int, index of answer column\n",
    "    max_length: max_length after minusing [cls] and [sep] tokens, default: 512\n",
    "    overlap_rate: float, overlap rate between each snippet pieces, default 0.75\n",
    "    \n",
    "    return:\n",
    "    expanded\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    # iterate through all entries in df\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        one_entry = df.iloc[i,:] \n",
    "\n",
    "        # calculate length for question, 5 choices, snippet length\n",
    "        q_len = len(one_entry['q'])\n",
    "        c_len_list = [len(str(c)) for c in one_entry[one_entry.index.isin(['c1', 'c2', 'c3', 'c4', 'c5'])].tolist()]\n",
    "        s1_len = len(str(one_entry['s1']))\n",
    "        \n",
    "        # for each entry, take its 5 choices in sequence into 5 [question, one_choice, label] outputs\n",
    "        for j in range(5):\n",
    "            choice_index = first_choice_index + j\n",
    "            remaining_len = max_length - q_len - c_len_list[j] # remaining index for snippet\n",
    "\n",
    "            label = 1 if (choice_index - first_choice_index + 1) == one_entry[answer_index] else 0\n",
    "            snippet_list = divide_snippet(one_entry['s1'], remaining_len, overlap_rate)\n",
    " \n",
    "            for k, snippet in enumerate(snippet_list):\n",
    "                \n",
    "                result.append({'question': one_entry[question_index], \n",
    "                       'choice': one_entry[choice_index], \n",
    "                       'label': label,\n",
    "                       'snippet': snippet,\n",
    "                       'qtype': df['qtype'][i],\n",
    "                       'q_index': i,\n",
    "                       'c_index': j,\n",
    "                       's_index': k})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_18_separate_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_separate_overlapped = pd.DataFrame(prepare_data_overlapped(test_data_17_separate_qs,0,1,8, \n",
    "                                                                   max_length=509,\n",
    "                                                                  overlap_rate=0.75))\n",
    "test_17_separate_overlapped = test_17_separate_overlapped[['q_index',\n",
    "                                                           'question',\n",
    "                                                           'c_index',\n",
    "                                                           'choice',\n",
    "                                                           's_index',\n",
    "                                                           'snippet',\n",
    "                                                           'label', \n",
    "                                                           'qtype']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_18_separate_overlapped = pd.DataFrame(prepare_data_overlapped(test_data_18_separate_qs,0,1,8, max_length=509, \n",
    "                                                                  overlap_rate=0.75))\n",
    "test_18_separate_overlapped = test_18_separate_overlapped[['q_index',\n",
    "                                                           'question',\n",
    "                                                           'c_index',\n",
    "                                                           'choice',\n",
    "                                                           's_index',\n",
    "                                                           'snippet',\n",
    "                                                           'label', \n",
    "                                                           'qtype']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3000_separate_overlapped = pd.DataFrame(prepare_data_overlapped(train_data_3000_separate_qs,0,2,7, max_length=509,\n",
    "                                                                     overlap_rate=0.75))\n",
    "train_3000_separate_overlapped = train_3000_separate_overlapped[['q_index',\n",
    "                                                           'question',\n",
    "                                                           'c_index',\n",
    "                                                           'choice',\n",
    "                                                           's_index',\n",
    "                                                           'snippet',\n",
    "                                                           'label', \n",
    "                                                           'qtype']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_separate_overlapped = pd.DataFrame(prepare_data_overlapped(train_data_separate_qs,0,2,7, max_length=509,\n",
    "                                                                     overlap_rate=0.75))\n",
    "train_separate_overlapped = train_separate_overlapped[['q_index',\n",
    "                                                           'question',\n",
    "                                                           'c_index',\n",
    "                                                           'choice',\n",
    "                                                           's_index',\n",
    "                                                           'snippet',\n",
    "                                                           'label', \n",
    "                                                           'qtype']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_separate_overlapped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save files to help calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the separate-overlapped files with to help calculating evaluation accuracy\n",
    "test_17_separate_overlapped.to_excel(f'./data/_output_data/{TEST_17_NAME}.xlsx', index=None)\n",
    "test_18_separate_overlapped.to_excel(f'./data/_output_data/{TEST_18_NAME}.xlsx', index=None)\n",
    "train_3000_separate_overlapped.to_excel(f'./data/_output_data/{TRAIN_3000_NAME}.xlsx', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffling  and combine question & snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_df(df, frac=1, random_state=42):\n",
    "    return df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined \n",
    "# train_combine_qs_shuffled = shuffle_df(train_df_combined_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_length(df):\n",
    "    total_length = [len(str(i)+str(j)+str(k)) for i, j, k in zip(df['question'], \n",
    "                                       df['snippet'], \n",
    "                                       df['choice'])]\n",
    "    \n",
    "    max_length = max(total_length)\n",
    "    avg_length = sum(total_length) // len(total_length)\n",
    "    assert max_length <= 512\n",
    "    print(f'Max len: {max_length} <= 512!, Avg len: {avg_length} --UPDATE NEEDED if using over two [SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_question_snippet(df):\n",
    "    evaluate_length(df)\n",
    "    \n",
    "    combined_qs = [i+j for i, j in zip(df['question'], \n",
    "                                   df['snippet'])]\n",
    "    df_combined = df.copy()\n",
    "    df_combined['combined_qs'] = pd.Series(combined_qs)\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_overlapped = combine_question_snippet(train_separate_overlapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_overlapped_shuffled = shuffle_df(train_combined_overlapped)\n",
    "train_combined_overlapped_shuffled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_combined_overlapped = combine_question_snippet(test_17_separate_overlapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_18_combined_overlapped = combine_question_snippet(test_18_separate_overlapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3000_combined_overlapped = combine_question_snippet(train_3000_separate_overlapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histgram(df, column_name, kde=False):\n",
    "    if column_name in df.columns:\n",
    "        print(f'{column_name} length')\n",
    "        data = [len(str(i)) for i in df[column_name]]\n",
    "        sns.distplot(data, kde=kde, label='snippet length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_histgram(train_combined_overlapped, 'question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_histgram(train_combined_overlapped, 'combined_qs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_bert(df):\n",
    "    return pd.DataFrame({\n",
    "        'id':range(df.shape[0]),\n",
    "        'label':df['label'],\n",
    "        'alpha':['a']*df.shape[0],\n",
    "        'text_a': df['combined_qs'].replace(r'\\n', ' ', regex=True),\n",
    "        'text_b': df['choice'].replace(r'\\n', ' ', regex=True)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_bert_combined_qs = prepare_for_bert(train_combine_qs_shuffled)\n",
    "#train_bert_combined_qs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_bert_combined_qs.to_csv('./data/combined_qs_title/train_combined_qs_title.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 3000 questions in training data\n",
    "#train_3000_bert_combined_qs = prepare_for_bert(train_3000_df_combined_qs)\n",
    "#train_3000_bert_combined_qs.to_csv('./data/combined_qs_title/train_3000_combined_qs_title.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training data\n",
    "#dev_17_bert_combined_qs = prepare_for_bert(test_17_df_combined_qs)\n",
    "#dev_18_bert_combined_qs = prepare_for_bert(test_18_df_combined_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_17_bert_combined_qs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_17_bert_combined_qs.to_csv('./data/combined_qs_title/dev_17_combined_qs_title.tsv', sep='\\t', index=False, header=False)\n",
    "#dev_18_bert_combined_qs.to_csv('./data/combined_qs_title/dev_18_combined_qs_title.tsv', sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert_combined_overlapped = prepare_for_bert(train_combined_overlapped_shuffled)\n",
    "train_bert_combined_overlapped.head()\n",
    "print(train_bert_combined_overlapped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert_combined_overlapped.to_csv(f'./data/combined_overlapped/{TRAIN_NAME}.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_17_bert_combined_overlapped = prepare_for_bert(test_17_combined_overlapped)\n",
    "test_17_bert_combined_overlapped.to_csv(f'./data/combined_overlapped/{TEST_17_NAME}.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_18_bert_combined_overlapped = prepare_for_bert(test_18_combined_overlapped)\n",
    "test_18_bert_combined_overlapped.to_csv(f'./data/combined_overlapped/{TEST_18_NAME}.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3000_bert_combined_overlapped = prepare_for_bert(train_3000_combined_overlapped)\n",
    "train_3000_bert_combined_overlapped.to_csv(f'./data/combined_overlapped/{TRAIN_3000_NAME}.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make nested logits \n",
    "def check_nested_logits(nested_logits, original_df):\n",
    "    result_struct = []\n",
    "    result_logits = []\n",
    "    for q, qq in enumerate(nested_logits):\n",
    "        for c, cc in enumerate(qq):\n",
    "            for s, ss in enumerate(cc):\n",
    "                result_struct.append([q, c, s])\n",
    "                result_logits.append(ss)\n",
    "    correct_struct = original_df[['q_index','c_index','s_index']].values.tolist()\n",
    "    correct_logits = original_df[['l_logits', 'r_logits']].values.tolist()\n",
    "\n",
    "    assert correct_struct.__eq__(result_struct), 'Wrong structure of nested logits'\n",
    "    assert correct_logits.__eq__(result_logits), 'Wrong logits value'\n",
    "    print('Structure and Logits values are the same')\n",
    "    \n",
    "def nest_output_logits(df):\n",
    "\n",
    "    nested_logits = []\n",
    "    cc_cache = []\n",
    "    qq_cache = []\n",
    "    df_length = df.shape[0]\n",
    "    qcs_indexes = df[['q_index', 'c_index', 's_index']].values\n",
    "    assert qcs_indexes.dtype == 'int64', 'Wrong indexes dtype, should be int'\n",
    "    input_logits = df[['l_logits', 'r_logits']].values\n",
    "    assert input_logits.dtype == 'float64', 'Wrong indexes dtype, should be float64' ## CHANGE THIS\n",
    "    \n",
    "    for i in range(df_length - 1):\n",
    "        q, c, s = qcs_indexes[i]\n",
    "        # l_logit, r_logit = input_logits[i]\n",
    "        #  logit = [l_logit, r_logit]\n",
    "        logit = input_logits[i].tolist()\n",
    "        nq, nc, ns = qcs_indexes[i+1]\n",
    "        if c == nc: # same choice \n",
    "            cc_cache.append(logit)\n",
    "        elif c != nc and q == nq: # not same choice, still in the same question\n",
    "            cc_cache.append(logit)\n",
    "            qq_cache.append(cc_cache)\n",
    "            cc_cache = []\n",
    "        else: # not same question\n",
    "            cc_cache.append(logit)\n",
    "            qq_cache.append(cc_cache)\n",
    "            nested_logits.append(qq_cache)\n",
    "            cc_cache = []\n",
    "            qq_cache = []\n",
    "        if i == df_length - 2:\n",
    "            # last loop\n",
    "            assert i + 1 == df_length - 1\n",
    "            last_logit = input_logits[i+1].tolist()\n",
    "            if c == nc:\n",
    "                cc_cache.append(last_logit)\n",
    "            else:\n",
    "                cc_cache.append(last_logit)\n",
    "            qq_cache.append(cc_cache)\n",
    "            nested_logits.append(qq_cache)\n",
    "            cc_cache = []\n",
    "    check_nested_logits(nested_logits, df)\n",
    "    return nested_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_18 = pd.read_excel('outputs/test_18_75%.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_logits_18 = nest_logits(output_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = output_18.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_test = nest_logits(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First method: avg in choice level\n",
    "\n",
    "nested_logits -> choice_level_average_logits -> diff -> question_level_max -> label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q, qq in enumerate(nested_test):\n",
    "    for c, cc in enumerate(qq):\n",
    "        print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_score(y_true, y_pred):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    \n",
    "    for i, true in enumerate(y_true):\n",
    "        if true == y_pred[i]:\n",
    "            if true == 1:\n",
    "                tp += 1\n",
    "            else: # == 0\n",
    "                tn += 1\n",
    "        else:\n",
    "            if true == 1:\n",
    "                fn += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "    true_false_values = {'tp:': tp,'tn:': tn, 'fp:': fp, 'fn:': fn}\n",
    "    print(true_false_values)\n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    return {\n",
    "        \"one_entry_acc\": round(accuracy, 3),\n",
    "        \"precision\": round(precision, 3),\n",
    "        \"recall\": round(recall, 3),\n",
    "        \"values\": true_false_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_logits = output_18[['l_logits', 'r_logits']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.argmax(correct_logits, axis=1) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_entry_preds = np.argmax(correct_logits, axis=1)\n",
    "one_entry_label = output_18['label'].values\n",
    "precision_recall_score(one_entry_label, one_entry_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_logits(cc):\n",
    "    cc_np = np.array(cc)\n",
    "    #print('hi')\n",
    "    # assert cc_np.dtype == 'float64'\n",
    "    avg_logits = np.mean(cc_np, axis=0)\n",
    "    #print('hihii')\n",
    "    print(avg_logits)\n",
    "    return avg_logits\n",
    "\n",
    "def qcs_pair_accuracy():\n",
    "    pass\n",
    "question_level_logits = []\n",
    "for q, qq in enumerate(nested_logits_18):\n",
    "    choice_level_logits = []\n",
    "    for c, cc in enumerate(qq):\n",
    "        print(cc)\n",
    "        snippet_avg_logits = calc_avg_logits(cc)\n",
    "        choice_level_logits.append(snippet_avg_logits)\n",
    "    question_level_logits.extend(choice_level_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_level_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(question_level_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(question_avg_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_18[['q_index', 'c_index','label']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_18[['q_index', 'c_index','label']].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs = np.unique(output_18[['q_index', 'c_index', 'label']].values, axis=0)\n",
    "question_level_label = [label for _, _, label in unique_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_level_preds = np.argmax(question_level_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_score(question_level_label, question_level_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_level_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! delete\n",
    "def question_accuracy_old(raw_preds, out_label_ids):\n",
    "    \"\"\"\n",
    "    raw_preds: the value from logits, namely preds before argmax\n",
    "    out_label_ids: the [0, 1] value for each question-choice pair\n",
    "    \n",
    "    return: question accuracy\n",
    "    \"\"\"\n",
    "    def accuracy(labels, preds):\n",
    "        length = len(labels)\n",
    "        correct_num = 0\n",
    "        for i in range(length):\n",
    "            if labels[i] == preds[i]:\n",
    "                correct_num += 1\n",
    "        return correct_num / length\n",
    "    \n",
    "    # find true labels for questions\n",
    "    labels = []\n",
    "    question_number = int(len(out_label_ids) / 5)\n",
    "    for question in range(question_number):\n",
    "        for choice in range(5):\n",
    "            choice_index = 5*question + choice\n",
    "            if out_label_ids[choice_index] == 1:\n",
    "                labels.append(choice_index % 5 + 1)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    # find predicted labels for questions \n",
    "    predicted_labels = []\n",
    "    for question in range(question_number):\n",
    "        # print('question number: ', question)\n",
    "        temp = []\n",
    "        for choice in range(5):\n",
    "            # starting choice index: 5*question + choice\n",
    "            # ending choice index: 5*question + choice\n",
    "            choice_index = 5*question + choice\n",
    "            cur_choice_preds = raw_preds[choice_index] # [0.81673616, -0.56396836]\n",
    "            # print(preds[choice_index])\n",
    "            temp.append(cur_choice_preds[0] - cur_choice_preds[1])\n",
    "\n",
    "        result_index = np.argmin(temp)\n",
    "        predicted_labels.append(result_index + 1)\n",
    "    return accuracy(labels, predicted_labels)\n",
    "\n",
    "\n",
    "\n",
    "def question_accuracy(qs_pair_label, qs_pair_logits):\n",
    "    \"\"\"\n",
    "    qs_pair_logits: the value from logits, eg [0.288778692483902, -1.444930672645569]\n",
    "    qs_pair_label: the [0, 1] value for each question-choice pair\n",
    "    \n",
    "    return: question accuracy\n",
    "    \"\"\"\n",
    "    def accuracy(labels, preds):\n",
    "        length = len(labels)\n",
    "        correct_num = 0\n",
    "        for i in range(length):\n",
    "            if labels[i] == preds[i]:\n",
    "                correct_num += 1\n",
    "        return correct_num / length\n",
    "    \n",
    "\n",
    "    # find true labels for questions\n",
    "    labels = []\n",
    "    question_number = int(len(qs_pair_label) / 5)\n",
    "    for question in range(question_number):\n",
    "        for choice in range(5):\n",
    "            choice_index = 5*question + choice\n",
    "            if qs_pair_label[choice_index] == 1:\n",
    "                labels.append(choice_index % 5 + 1)\n",
    "                break\n",
    "    # int(labels)\n",
    "    \n",
    "    # find predicted labels for questions \n",
    "    predicted_labels = []\n",
    "    for question in range(question_number):\n",
    "        # print('question number: ', question)\n",
    "        temp = []\n",
    "        for choice in range(5):\n",
    "            # starting choice index: 5*question + choice\n",
    "            # ending choice index: 5*question + choice\n",
    "            choice_index = 5*question + choice\n",
    "            cur_choice_preds = qs_pair_logits[choice_index] # [0.81673616, -0.56396836]\n",
    "            # print(preds[choice_index])\n",
    "            temp.append(cur_choice_preds[0] - cur_choice_preds[1])\n",
    "\n",
    "        result_index = np.argmin(temp)\n",
    "        predicted_labels.append(result_index + 1)\n",
    "    # ? MAYBE CAN WRITE SOME TESTING METHOD\n",
    "    print('predicted_labels: ')\n",
    "    print(predicted_labels)\n",
    "    print('labels ---')\n",
    "    print(labels)\n",
    "    return accuracy(labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_entry_score(output_df):\n",
    "    \"\"\"\n",
    "    output_df: dataframe, output with logits \n",
    "    \n",
    "    return: precision, recall, question_choice_pair accuracy and old accuracy \n",
    "    \"\"\"\n",
    "    # Evaluate one entry acc, precision, recall\n",
    "    one_entry_logits = output_df[['l_logits', 'r_logits']].values.tolist()\n",
    "    one_entry_preds = np.argmax(one_entry_logits, axis=1)\n",
    "    one_entry_label = output_df['label'].values\n",
    "    result = precision_recall_score(one_entry_label, one_entry_preds)\n",
    "    # ! FIXME: update needed \n",
    "    result['old_question_acc'] = question_accuracy_old(one_entry_logits, one_entry_label)\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_question_score(output_df, nested_logits):\n",
    "    # first method - question level logits\n",
    "    \"\"\"\n",
    "    output_df: dataframe, output with logits \n",
    "    nested_logits: list\n",
    "\n",
    "    return: question level accuracy \n",
    "    \"\"\"\n",
    "    question_level_logits = first_method_avg(nested_logits)\n",
    "\n",
    "    # question level true labels\n",
    "    unique_pairs = np.unique(output_df[['q_index', 'c_index', 'label']].values, axis=0)\n",
    "    question_level_label = [label for _, _, label in unique_pairs]\n",
    "    return question_accuracy(question_level_label, question_level_logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_one_entry_score(output_18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second method avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first method \n",
    "def calc_avg_logits(cc):\n",
    "    cc_np = np.array(cc)\n",
    "    #print('hi')\n",
    "    # assert cc_np.dtype == 'float64'\n",
    "    avg_logits = np.mean(cc_np, axis=0)\n",
    "    #print('hihii')\n",
    "    # print(avg_logits)\n",
    "    return avg_logits\n",
    "\n",
    "def second_method_avg(nested_logits):\n",
    "    \"\"\"\n",
    "    nested_logits: list, nested logits grouped by question and choice level \n",
    "    Average logits for all snippet pieces in a question-choice pair \n",
    "    \"\"\"\n",
    "    question_level_logits = []\n",
    "    for q, qq in enumerate(nested_logits):\n",
    "        choice_level_logits = []\n",
    "        for c, cc in enumerate(qq):\n",
    "            # print(cc)\n",
    "            snippet_avg_logits = calc_avg_logits(cc)\n",
    "            choice_level_logits.append(snippet_avg_logits)\n",
    "        question_level_logits.extend(choice_level_logits)\n",
    "    return question_level_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_question_score(output_18, nested_logits_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = output_18.head(35)\n",
    "nested_test = nest_output_logits(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First method max, no need to have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_method_max(nested_logits):\n",
    "    \"\"\"\n",
    "    return: predicted labels in [1,5]\n",
    "    \"\"\"\n",
    "    predicted_label = []\n",
    "\n",
    "\n",
    "    for q, qq in enumerate(nested_logits):\n",
    "        max_diff = -2**16 # correctness\n",
    "        label = -1\n",
    "\n",
    "        for c, cc in enumerate(qq):\n",
    "            #print('choice:', c)\n",
    "            #print(cc)\n",
    "            for s in cc:\n",
    "                # print(s)\n",
    "                cur_diff = s[1] - s[0]\n",
    "                # print(cur_diff)\n",
    "                if cur_diff > max_diff:\n",
    "                    max_diff = cur_diff\n",
    "                    label = c + 1\n",
    "                # cur_diff = ss[s][1] - ss[s][0]\n",
    "                # print(cur_diff)\n",
    "\n",
    "            # snippet_avg_logits = calc_avg_logits(cc)\n",
    "        predicted_label.append(label)\n",
    "        # print('label is:', label)\n",
    "    print(predicted_label)\n",
    "    return predicted_label\n",
    "\n",
    "def accuracy(labels, preds):\n",
    "    length = len(labels)\n",
    "    correct_num = 0\n",
    "    for i in range(length):\n",
    "        if labels[i] == preds[i]:\n",
    "            correct_num += 1\n",
    "    return correct_num / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_18 = first_method_max(nested_logits_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [2, 3, 2, 5, 3, 3, 5, 1, 2, 4, 4, 1, 4, 1, 1, 3, 2, 3, 3, 2, 1, 1, 4, 4, 3, 4, 5, 2, 1, 3, 2, 1, 3, 2, 5, 4, 1, 5, 3, 4, 3, 1, 3, 5, 3, 1, 3, 4, 4, 2, 4, 2, 3, 3, 4, 5, 2, 2, 3, 3, 4, 2, 4, 1, 1, 5, 5, 2, 2, 2, 1, 5, 1, 2, 2, 1, 2, 1, 1, 3, 2, 4, 4, 2, 4, 2, 4, 2, 1, 4, 2, 5, 1, 5, 4, 1, 3, 3, 5, 1, 2, 1, 1, 3, 2, 3, 1, 1, 5, 3, 5, 2, 5, 5, 1, 5, 2, 5, 3, 4, 1, 4, 3, 2, 1, 2, 2, 5, 1, 4, 3, 5, 3, 1, 5, 3, 1, 5, 4, 2, 3, 2, 2, 4, 5, 4, 3, 2, 2, 1, 3, 3, 5, 5, 2, 4, 3, 3, 4, 5, 5, 4, 3, 2, 4, 2, 4, 5, 3, 3, 5, 1, 4, 2, 2, 2, 3, 1, 4, 1, 4, 3, 3, 5, 5, 4, 1, 2, 3, 3, 1, 5, 5, 5, 4, 2, 4, 2, 2, 4, 5, 5, 2, 5, 2, 1, 5, 4, 2, 4, 1, 3, 4, 2, 1, 3, 3, 1, 1, 1, 5, 2, 1, 4, 1, 1, 5, 1, 1, 4, 2, 5, 2, 3, 4, 5, 3, 4, 3, 4, 3, 3, 3, 5, 3, 3, 4, 5, 5, 2, 1, 3, 1, 5, 1, 1, 4, 5, 4, 4, 5, 5, 3, 5, 1, 4, 4, 3, 2, 3, 5, 4, 4, 4, 4, 5, 3, 1, 1, 5, 4, 1, 3, 3, 1, 2, 4, 2, 1, 3, 1, 5, 5, 3, 1, 4, 4, 1, 5, 4, 4, 5, 1, 4, 1, 1, 1, 4, 3, 4, 2, 4, 3, 4, 2, 5, 3, 4, 1, 1, 5, 5, 4, 1, 1, 1, 1, 3, 2, 5, 2, 2, 3, 3, 2, 2, 1, 4, 5, 5, 4, 4, 3, 4, 2, 1, 5, 4, 1, 2, 5, 1, 1, 2, 5, 1, 1, 4, 3, 4, 1, 5, 4, 2, 4, 4, 3, 2, 1, 4, 4, 2, 3, 3, 5, 1, 1, 4, 3, 1, 2, 4, 5, 3, 2, 3, 4, 2, 5, 3, 5, 2, 4, 1, 2, 2, 1, 4, 4, 4, 5, 1, 3, 4, 4, 1, 4, 4, 2, 1, 2, 3, 1, 1, 1, 5, 4, 2, 2, 3, 2, 3, 4, 1, 5, 5, 5, 4, 2, 4, 4, 1, 4, 1, 2, 1, 2, 2, 4, 3, 4, 3, 2, 5, 4, 2, 1, 2, 3, 2, 2, 4, 1, 3, 3, 3, 3, 3, 3, 3, 5, 1, 3, 5, 2, 2, 2, 2, 2, 5, 3, 2, 4, 5, 5, 2, 1, 2, 3, 3, 3, 2, 5, 5, 2, 4, 5, 5, 4, 3, 1, 5, 4, 2, 1, 1, 3, 5, 2, 4, 5, 4, 3, 1, 3, 5, 4, 1, 1, 4, 2, 3, 4, 4, 3, 2, 3, 1, 3, 3, 1, 2, 1, 2, 2, 1, 3, 2, 5, 3, 1, 5, 4, 4, 2, 4, 1, 1, 5, 1, 1, 2, 2, 3, 5, 1, 4, 2, 4, 5, 4, 1, 5, 5, 5, 2, 4, 2, 2, 3, 3, 4, 3, 5, 4, 3, 4, 5, 1, 5, 4, 4, 2, 2, 2, 4, 1, 3, 2, 5, 2, 3, 2, 2, 2, 2, 5, 4, 5, 3, 4, 3, 5, 5, 3, 4, 4, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(labels, predicted_18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third method - only correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = output_18.head(20)\n",
    "nested_test = nest_output_logits(test)\n",
    "nested_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snippet_avg_helper(qcs_level_diff):\n",
    "    \"\"\"\n",
    "    qcs_level_diff: difference between the right logit and left logit for each choice snippet. This mesures \n",
    "    the correctness of the question_choice_snippet. \n",
    "    \n",
    "    correctness <= 0, wrong \n",
    "    correctness > 0, correct \n",
    "    \n",
    "    return: label in [1,5] for a question \n",
    "    \"\"\"\n",
    "    qc_level_avg = []\n",
    "    for c, cc in enumerate(qcs_level_diff):\n",
    "        correct_pair_nums = len([s for s in cc if s > 0])\n",
    "        wrong_pair_nums = len([s for s in cc if s <= 0])\n",
    "        \n",
    "        correct_pair_avg = 0 if correct_pair_nums == 0 else sum([s for s in cc if s > 0]) / correct_pair_nums\n",
    "        wrong_pair_avg = 0 if wrong_pair_nums == 0 else sum([s for s in cc if s <= 0]) / wrong_pair_nums\n",
    "        print(correct_pair_avg, wrong_pair_avg)\n",
    "        \n",
    "        # Method FOUR: both correct and wrong \n",
    "        correct_and_wrong_avg = (correct_pair_avg + wrong_pair_avg) / 2\n",
    "        print(correct_and_wrong_avg)\n",
    "        qc_level_avg.append(correct_and_wrong_avg)\n",
    "    print('Avg for a5 choices in a question', qc_level_avg)\n",
    "    print(qc_level_avg.index(max(qc_level_avg)))\n",
    "    return qc_level_avg.index(max(qc_level_avg)) + 1\n",
    "        \n",
    "        \n",
    "q_labels_preds = []\n",
    "for q in nested_logits_18:\n",
    "    # print('question:::----', q)\n",
    "    choice_level = []\n",
    "    for c in q:\n",
    "        print('choice----', c)\n",
    "        snippet_level = []\n",
    "        for s in c:\n",
    "            # print('snippet--', s)\n",
    "            diff = s[1] - s[0]\n",
    "            # print(diff)\n",
    "            snippet_level.append(diff)\n",
    "        #print(snippet_level)\n",
    "        # print('')\n",
    "        choice_level.append(snippet_level)\n",
    "    \n",
    "    print('')\n",
    "    print(choice_level)\n",
    "    q_label = snippet_avg_helper(choice_level)\n",
    "    q_labels_preds.append(q_label)\n",
    "    print('')\n",
    "    print('')\n",
    "print(q_labels_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(labels,q_labels_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[-2.6316429376602177, -2.615735948085785, -2.1325479149818416], [-1.486147284507751, -1.144922822713852, -1.0900460630655289], [-0.463565394282341, -0.09689867496490481, -0.7583723440766335], [-2.258228838443756, -2.407961905002594, -1.6885483264923093], [-2.1981219649314876, -2.1288265585899353, -1.8730788230896]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = [[2.6316429376602177, 1.2, -2.1325479149818416],\n",
    " [-1.486147284507751, 10.144922822713852, 1.0900460630655289],\n",
    " [-0.463565394282341, -0.09689867496490481, -0.7583723440766335],\n",
    " [-2.258228838443756, -2.407961905002594, -1.6885483264923093],\n",
    " [-2.1981219649314876, -2.1288265585899353, -1.8730788230896]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_avg_helper(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snippet_avg_helper(choices):\n",
    "    for c, cc in enumerate(choices):\n",
    "        correct_nums = len([s for s in cc if s > 0])\n",
    "        wrong_nums = len([s for s in cc if s <= 0])\n",
    "        \n",
    "        correct_avg = 0 if correct_nums == 0 else sum([s for s in cc if s > 0]) / correct_nums\n",
    "        wrong_avg = 0 if wrong_nums == 0 else sum([s for s in cc if s <= 0]) / wrong_nums\n",
    "        print(correct_avg, wrong_avg)\n",
    "        \n",
    "\n",
    "snippet_avg_helper(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snippet_avg_helper_correct_only(qcs_level_diff, only_correct=True):\n",
    "    \"\"\"\n",
    "    qcs_level_diff: difference between the right logit and left logit for each choice snippet. This mesures \n",
    "    the correctness of the question_choice_snippet. \n",
    "    \n",
    "    correctness <= 0, wrong \n",
    "    correctness > 0, correct \n",
    "    \n",
    "    return: label in [1,5] for a question \n",
    "    \"\"\"\n",
    "    qc_level_avg = []\n",
    "    for c, cc in enumerate(qcs_level_diff):\n",
    "        correct_pair_nums = len([s for s in cc if s > 0])\n",
    "        wrong_pair_nums = len([s for s in cc if s <= 0])\n",
    "        \n",
    "        correct_pair_avg = 0 if correct_pair_nums == 0 else sum([s for s in cc if s > 0]) / correct_pair_nums\n",
    "        wrong_pair_avg = 0 if wrong_pair_nums == 0 else sum([s for s in cc if s <= 0]) / wrong_pair_nums\n",
    "        print(correct_pair_avg, wrong_pair_avg)\n",
    "        if only_correct:\n",
    "            correct_avg = correct_pair_avg if correct_pair_nums != 0 else wrong_pair_avg\n",
    "        # Method FOUR: both correct and wrong \n",
    "        # correct_and_wrong_avg = (correct_pair_avg + wrong_pair_avg) / 2\n",
    "        print(correct_avg)\n",
    "        qc_level_avg.append(correct_avg)\n",
    "    print('Avg for a5 choices in a question', qc_level_avg)\n",
    "    print(qc_level_avg.index(max(qc_level_avg)))\n",
    "    return qc_level_avg.index(max(qc_level_avg)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_avg_helper(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_avg_helper_correct_only(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
